{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"HIlMQWUiUqxO"},"source":["# Computer vision and deep learning - Laboratory 4\n"," \n","In this laboratory we'll work with a semantic segmentation model. The task of semantic segmentation implies the labeling/classification of __all__ the pixels in the input image. So, in this case, the output is not a single class label, as for classification, but an 2D array.\n"," \n","For the next two labs, you'll build and train a fully convolutional neural network inspired by U-Net. Finally, you'll implement several metrics suitable for evaluating segmentation models.\n","\n","Today, we'll focus on the data loading and preprocessing part, and we'll study the building blocks of the sematic segmentation module.\n"]},{"cell_type":"code","metadata":{"id":"wYMNqbIyVx0R","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668409978379,"user_tz":-120,"elapsed":7976,"user":{"displayName":"Adrian Astăluș","userId":"14912647310664374882"}},"outputId":"501209f7-d539-4928-aac6-20892e1c1981"},"source":["!pip install wget\n","\n","import os\n","import cv2\n","import wget\n","import glob\n","import torch\n","import shutil\n","import numpy as np\n","\n","import matplotlib.pyplot as plt\n"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting wget\n","  Downloading wget-3.2.zip (10 kB)\n","Building wheels for collected packages: wget\n","  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9675 sha256=3e39df938f75937aa15c78db1e92252f805e43b3d085eceaa97b49c132d71eac\n","  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n","Successfully built wget\n","Installing collected packages: wget\n","Successfully installed wget-3.2\n"]}]},{"cell_type":"markdown","metadata":{"id":"J_ZDW2dzV67G"},"source":["## Data loading\n"," \n","You will work with the OxfordPets dataset; we are aware that this dataset is also present in _torchvision_ module, but in this laboratory you are required to write the data loading from scratch. \n","\n","Each image has a segmentation mask assigned (with the same size as the input image); three classes are defined on each segmentation mask:\n","- Label 1: pet;\n","- Label 3: border of the pet;\n","- Label 2: background.\n"," \n","First let's write the code that will allow us to load this data.\n","As you remember from the previous lab, in _torch_ you have two data primitives that allow you to interact with the data: torch.utils.data.Dataset and torch.utils.data.DataLoader.\n","\n","torch.utils.data.Dataset stores the samples and their corresponding labels, and DataLoader wraps an iterable around the Dataset to enable easy access to the samples.\n","\n","[More details in the doc.](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)\n","\n","\n"]},{"cell_type":"markdown","source":["## Writing a custom Dataset\n","\n","In the file oxford_pets.py you have the boilerplate code for creating the custom dataset for this image segmentation problem.\n","\n","Each custom dataset must implement the following methods:\n","- \\_\\_init\\_\\_ : the constructor is run when instantiating the Dataset object. Here you should do the initializations (input dirs, dataset splits) and the transforms (covered in more detail in the next section).\n","- \\_\\_len\\_\\_ : this should return the number of samples in the dataset;\n","- \\_\\_getitem\\_\\_ : this should load and return a sample from the dataset at the given index _idx_ (passed as a parameter). Based on the index, it identifies the image and its corresponding segmentation map location on disk, calls the transform functions on them (if applicable), and returns the tensor image and mask in a tuple."],"metadata":{"id":"orPA7LTRRidd"}},{"cell_type":"code","metadata":{"id":"EO1_Wd_debx6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668409989118,"user_tz":-120,"elapsed":5837,"user":{"displayName":"Adrian Astăluș","userId":"14912647310664374882"}},"outputId":"1bc8f876-da4c-4f4b-8a3c-285c4c35e6d4"},"source":["!pip install wget\n","import os\n","import cv2\n","import wget\n","import torch\n","import random\n","import tarfile\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torchvision\n","\n","class OxfordPets(torch.utils.data.Dataset):\n","    _URLS = [\n","        \"https://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\"\n","        \"https://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz\"\n","    ]\n","    TARGET_SIZE = (224, 224)\n","    \"\"\"\n","    OxfordPets segmentation dataset\n","    :param root_dir (string) - the root directory where the data is store\n","    :param is_train (bool) - whether to use the train or the test split of the data\n","    :param transforms (callable, optional): A function/transform that takes as input a sample and returns its\n","        transformed version (like a horizontal flip)\n","    :param download (bool, optional): whether to download the data\n","    \"\"\"\n","    def __init__(self, root_dir, is_train, transforms=None, download=True):\n","        self.root_dir = root_dir\n","        self.transforms = transforms\n","\n","        # if needed, download the dataset\n","        if download:\n","            self.download_dataset(root_dir)\n","\n","        # the images are stored in the 'images' subfolder from root_dir as jpg file\n","        self.images_folder = os.path.join(self.root_dir, 'images')\n","        # the corresponding segmentation maps are stored in the 'annotations/trimaps'\n","        # subfolder from root_dir as png files\n","        anno_folder = os.path.join(self.root_dir, 'annotations')\n","        self.segmentations_folder = os.path.join(anno_folder, 'trimaps')\n","\n","        # the dataset is already divided into train/test splits\n","        # these are stored in the 'annotations' subfolder in trainval.txt and test.txt respectively\n","        splits_file = os.path.join(anno_folder, 'trainval.txt' if is_train else 'test.txt')\n","        # TODO your code here\n","        # in these files, on each line you have the information about an image\n","        # split each line by spaces and take only the image name and store it to image_ids\n","        image_ids = []\n","\n","        # then populate the lists images and segmentations with the full path of the images and their seg maps\n","        # for position i on this lists, you will have the ith image (images[i])\n","        # and its corresponding segmentation map (segmentations[i])\n","        # keep in mind that the images are jpg files stored in self.images_folder\n","        # and that the segmentation maps are png files stored in self.segmentation_folder\n","        self.images = []\n","        self.segmentations = []\n","        # end TODO your code here\n","\n","    def __getitem__(self, idx: int):\n","        \"\"\"\n","        Returns the idx-th sample from the dataset and its corresponding segmentation map\n","        \"\"\"\n","        # TODO your code here\n","        # load the image and the segmentation map from position idx\n","        image = None\n","        seg = None # read the segmentation as grayscale\n","        # preprocess the segmentation mask using the preprocess_segmentation function\n","        # resize the mask and the input image to OxfordPets.TARGET_SIZE (use make_image_square and resize)\n","        # think about what interpolation should you be using when resizing the mask\n","        # if self.transfroms is not None, apply the transformation function\n","        # return a dictionary with the image and the segmentation map\n","        # {'image': None, 'segmentation': None}\n","        return None, None\n","\n","    @staticmethod\n","    def preprocess_segmentation(mask: np.ndarray) ->np.ndarray:\n","        \"\"\"\"\n","        Preprocesses the segmentation mask such that the background pixels have a value of 0\n","        and the pet and border pixels have a value of 1\n","        :param segmentation mask\n","        \"\"\"\n","        # TODO your code here\n","        # the pixels that have a value of 2 (background) should be set to 0\n","        # the pixels that have a value of 1 or 3 (pet or pet border) should be set to 1\n","        return None\n","\n","    @staticmethod\n","    def make_image_square(img: np.ndarray, padding_mode: str = 'edge', padding_value: int = 0):\n","        \"\"\"\"\n","        Resizes the image such that it has an aspect ratio of 1.\n","        The smallest dimension is padded such that it has the same dimension as the largest dimension\n","        :param img - input image\n","        :param padding_mode - string descrbing the padding strategy (can be constant or edge).\n","                edge - the first and last rows and columns are replicated\n","                constants - the image is padded with constant values\n","        :param padding_value - in case of constant padding, the value used to pad the image.\n","        \"\"\"\n","        # TODO your code here\n","        # determine the padding value\n","        padding = -1\n","        # use np.pad to pad the image\n","        # return the padded image\n","        return None\n","\n","\n","    @staticmethod\n","    def resize_image(img: np.ndarray, target_size: int = (224, 224), interpolation: int = cv2.INTER_LINEAR):\n","        \"\"\"\"\n","        Resizes the image to the specified size\n","        :param img - input image\n","        :param target_size - the requested size to which the image should be resized to)\n","        :param interpolation - interpolation type\n","        \"\"\"\n","        # TODO your code here\n","        # resize the input image and return the result\n","        # cv2.resize might prove useful\n","        return None\n","\n","\n","    def __len__(self) -> int:\n","        \"\"\"\"\n","        Returns the size of this dataset (the number of images in the dataset)\n","        \"\"\"\n","        # TODO your code here : return the number of images in the dataset\n","        return -1\n","\n","    def download_dataset(self, root_dir: str):\n","        \"\"\"\"\n","        Downloads the OxfordPets images and annotations and saves them to root_dir\n","        :param root_dir(string): the directory where the data is downloaded\n","        \"\"\"\n","        if not os.path.exists(root_dir):\n","            os.makedirs(root_dir)\n","\n","            for url in OxfordPets._URLS:\n","                archive = wget.download(url, root_dir)\n","                with tarfile.open(archive) as archive_file:\n","                    archive_file.extractall(root_dir)\n","                os.remove(archive)\n","        else:\n","            print('Folder already exists, skipping download')"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: wget in /usr/local/lib/python3.7/dist-packages (3.2)\n"]}]},{"cell_type":"code","source":["# let's test the code\n","root_dir = 'oxford_pets'\n","training_data = OxfordPets(\n","    root_dir=root_dir,\n","    is_train=True,\n","    download=True, transforms=torchvision.transforms.Compose([RandomCrop(200)])\n",")\n","\n","figure = plt.figure(figsize=(8, 8))\n","cols, rows = 4, 4\n","for i in range(1, cols * rows + 1, 2):\n","    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n","    sample = training_data[sample_idx]\n","    img, seg = sample['image'], sample['segmentation']*120\n","    figure.add_subplot(rows, cols, i)\n","    plt.title('image')\n","    plt.axis(\"off\")\n","    plt.imshow(img)\n","\n","    figure.add_subplot(rows, cols, i+1)\n","    plt.title('seg')\n","    plt.axis(\"off\")\n","    plt.imshow(seg, cmap='gray')\n","plt.show()\n","\n","test_data = OxfordPets(\n","    root_dir=root_dir,\n","    is_train=False,\n","    download=True,\n",")"],"metadata":{"id":"dHvWh4cqX_RP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Inspect the pixels values that you have in a segmentation mask."],"metadata":{"id":"tJ-ch1SwenNT"}},{"cell_type":"code","source":["# TODO your code here"],"metadata":{"id":"76qQ0u7Bes7T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Transforms\n","\n","Often you want to apply some transformations on the input data (for example bring them to a pre-established shape, normalizing them, converting them to tensor) or you need to apply some augmentation techniques.\n","\n","This can be easily achieved via the _transforms_ callable that you sent to the constructor of the dataset (and applied it in _get_item()_).\n","Moreover, _torch_ offers an easy way to compose transforms by torchvision.transforms.Compose callable class which allows you to chain several transforms.\n","\n","Let's implement a simple augmentation, in which you randomly crop a region from the image. As you might notice, in the case of image segmentation, if we crop the input image, we must also crop the segmentation mask. "],"metadata":{"id":"eHl65KX5iuxY"}},{"cell_type":"code","source":["class RandomCrop(object):\n","    \"\"\"Crop randomly the image and the segmentation mask\n","\n","    param: output_size (tuple or int): requested output size.\n","           if int, square crop is made.\n","    \"\"\"\n","\n","    def __init__(self, output_size):\n","        assert isinstance(output_size, (int, tuple))\n","        if isinstance(output_size, int):\n","            self.output_size = (output_size, output_size)\n","        else:\n","            assert len(output_size) == 2\n","            self.output_size = output_size\n","\n","    def __call__(self, sample):\n","        image, segmentation = sample['image'], sample['segmentation']\n","        h, w = image.shape[:2]\n","        new_h, new_w = self.output_size\n","        if h < new_w or w < new_w:\n","            return sample\n","\n","        # TODO your code here\n","        # randomly generate the coordinates for the top left of the crop\n","        top = None\n","        left = None\n","        # crop both the image and the segmentation mask\n","        image = None\n","        segmentation = None\n","        return {'image': image, 'segmentation': segmentation}\n","\n"],"metadata":{"id":"mCOSSn8voKaC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, when you create the Dataset and this transform that you just wrote and analyse its effect."],"metadata":{"id":"U0x2W3XToi7K"}},{"cell_type":"code","source":["# TODO your code here\n","# hint: the code is aleary written for you (see above), you just need to pass the trasnsform"],"metadata":{"id":"tMG0bVycotJM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, let's write another class to transform the image and the segmentation map to tensors.\n","\n","Then, chain these transforms using torchvision.transforms.Compose."],"metadata":{"id":"ojmGo01co5-i"}},{"cell_type":"code","source":["class ToTensor(object):\n","    def __call__(self, sample):\n","        image, segmentation = sample['image'], sample['segmentation']\n","        # NEXT TIME\n","        # numpy image: H x W x C\n","        # torch image: C x H x W\n","        # image = image.transpose((2, 0, 1))\n","        return {'image': torch.from_numpy(image),\n","                'segmentation': torch.from_numpy(segmentation)}\n","\n"],"metadata":{"id":"cAZB0191pa4v"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Dataloaders\n","\n","By now you have a class that can easily retrieve one sample (image and segmentation map) at a time from your dataset. However, when training a model, you usually pass data in batches, you need to shuffle the data at each epoch and Python’s multiprocessing to speed up data retrieval.\n","Fortunately, all these are provided by the DataLoader.\n"],"metadata":{"id":"JQSmCD-7euux"}},{"cell_type":"code","source":["# let's create a DataLoader to easily iterate over this dataset\n","bs = 4\n","dataloader = torch.utils.data.DataLoader(training_data, batch_size=bs, shuffle=True, num_workers=0)\n","\n","for i_batch, sample_batched in enumerate(dataloader):\n","    imgs = sample_batched['image']\n","    segs = sample_batched['segmentation']\n","    print(i_batch, imgs.size(), segs.size())\n","\n","    rows, cols = bs, 2\n","    figure = plt.figure(figsize=(bs, 2))\n","    for i in range(0, bs):\n","        figure.add_subplot(rows, cols, 2*i+1)\n","        plt.title('image')\n","        plt.axis(\"off\")\n","        plt.imshow(imgs[i].numpy())\n","\n","        figure.add_subplot(rows, cols, 2*i+2)\n","        plt.title('seg')\n","        plt.axis(\"off\")\n","        plt.imshow(segs[i].numpy(), cmap=\"gray\")\n","    plt.show()\n","    # display the first 3 batches\n","    if i_batch == 2:\n","        break\n"],"metadata":{"id":"TFrgII4PfV6p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4cNuZRCCWBQo"},"source":["## Building the model\n"," \n","The model that will be used in this laboratory is inspired by the [U-Net](https://arxiv.org/abs/1505.04597) architecture.\n","U-Net is a fully convolutional neural network comprising two symmetric paths: a contracting path (to capture context) and an expanding path  (which enables precise localization). \n","The network also uses skip connections between the corresponding layers in the downsampling path to the layer in the upsampling path, and thus directly fast-forwards high-resolution feature maps from the encoder to the decoder network.\n","\n","The output of the model is an volume with depth C, where C is the number of pixel classes. For example, if you want to classify the pixels into pet and background, the output will be a volume of depth 2. \n","If you want to classify the pixels into pet, pet border and background the output will be a volume of depth 3.\n","\n","**Read the U-Net paper and try to understand the architecture.**\n"," \n","An overview of the U-Net architecture is depicted in the figure below:\n","<img src=\"https://miro.medium.com/max/1400/1*J3t2b65ufsl1x6caf6GiBA.png\"/>\n"," \n","\n"]},{"cell_type":"markdown","metadata":{"id":"VKKi5UowzvjZ"},"source":["## The downsampling path\n"," \n","\n","For the downsampling path we'll use a convolutional neural network from the pretrained torchvision models.\n","We'll cover this in detail in the next laboratory session.\n","\n","\n","## The upsamping path\n","\n","\n","In the upsampling path, we'll use transposed convolutions to progressively increase the resolution of the activation maps. The layers for the transposed convolution is [ConvTranspose2d](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html).\n"," \n","Let's write a function to implement an upsampling block, consisting of a transposed convolution, a batch normalization block and a ReLu activation.\n"," \n","Remember, the output size $W_o$ of a transposed convolutional layer is:  \n","\\begin{equation}\n","W_o = (W_i - 1) \\cdot S - 2P + F\n","\\end{equation},\n"," \n","where $W_i$ is the size of the input, $S$ is the stride, $P$ is the amount of padding and $F$ is the filter size.\n"," "]},{"cell_type":"code","source":["import torch\n","def upsample_block(x, filters, size, stride = 2):\n","  \"\"\"\n","  x - the input of the upsample block\n","  filters - the number of filters to be applied\n","  size - the size of the filters\n","  \"\"\"\n","\n","  # TODO your code here\n","  # transposed convolution\n","  # BN\n","  # relu activation\n","  return x"],"metadata":{"id":"eAyhAkgfCM4O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nOg9FKZmj7dH"},"source":["Now let's test this upsampling block"]},{"cell_type":"code","metadata":{"id":"wDy7Qkzmj6Tv","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c66b090f-2c73-4e44-ff57-4c9935227987"},"source":["in_layer = torch.rand((32, 32, 128, 128))\n","\n","filter_sz = 4\n","num_filters = 16\n","\n","for stride in [2, 4, 8]:\n","  x = upsample_block(in_layer, num_filters, filter_sz, stride)\n","  print('in shape: ', in_layer.shape, ' upsample with filter size ', filter_sz, '; stride ', stride, ' -> out shape ', x.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["in shape:  torch.Size([32, 32, 128, 128])  upsample with filter size  4 ; stride  2  -> out shape  torch.Size([32, 16, 258, 258])\n","in shape:  torch.Size([32, 32, 128, 128])  upsample with filter size  4 ; stride  4  -> out shape  torch.Size([32, 16, 512, 512])\n","in shape:  torch.Size([32, 32, 128, 128])  upsample with filter size  4 ; stride  8  -> out shape  torch.Size([32, 16, 1020, 1020])\n"]}]}]}